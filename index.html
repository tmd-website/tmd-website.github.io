<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="description" content="" />
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
        <script defer src="./static/js/fontawesome.all.min.js"></script>
        <link
            rel="stylesheet"
            href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-solarizedlight.min.css" />
        <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.1.0/prism-bibtex.min.js"></script>
        <link rel="stylesheet" href="static/css/index.css" />

        <title>Offline Goal Conditioned Reinforcement Learning with Temporal Distance Representations</title>

        <script
            type="text/javascript"
            id="MathJax-script"
            async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-svg.js"></script>
        <script>
            function selectContent(query) {
                var range = document.createRange()
                var selection = window.getSelection()
                var elem = document.querySelector(query)
                range.selectNodeContents(elem)
                selection.removeAllRanges()
                selection.addRange(range)
            }
        </script>
    </head>

    <body>
        <script type="text/javascript">
            window.MathJax = {
                tex: {
                    inlineMath: [
                        ["$", "$"],
                        ["\\(", "\\)"],
                    ],
                    displayMath: [
                        ["$$", "$$"],
                        ["\\[", "\\]"],
                    ],
                    processEscapes: true,
                    macros: {
                        S: "\\mathcal{S}",
                        A: "\\mathcal{A}",
                        D: "\\mathcal{D}",
                        R: "\\mathbb{R}",
                        E: "\\mathbb{E}",
                        P: "\\mathrm{P}",
                        var: "\\mathrm{Var}",
                        cov: "\\mathrm{Cov}",
                        argmin: "\\mathop{\\arg\\min}",
                        argmax: "\\mathop{\\arg\\max}",
                    },
                },
                svg: {
                    fontCache: "global",
                },
            }
        </script>
        <header>
            <h1>Offline Goal Conditioned Reinforcement Learning with Temporal Distance Representations</h1>
            <div class="authors">
                <span class="author">Anonymous Authors</span>
                <span class="affil">1</span>
            </div>
            <div class="notes">
                <span class="affil">1</span>
                <span class="institution">Anonymous Institution</span>
            </div>
            <div class="links">
                <span class="link">
                    <a href="./static/pdf/tmd.pdf" target="_blank" class="button">
                        <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
                    </a>
                </span>
                <!-- <span class="link"> -->
                <!--   <a href="TODO" class="button"> -->
                <!--     <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span> -->
                <!--   </a> -->
                <!-- </span> -->
                <span class="link">
                    <a href="https://anonymous.4open.science/r/ogcrl-FCDC/" class="button">
                        <span class="icon"><i class="fas fa-code"></i></span><span>Code</span>
                    </a>
                </span>
            </div>
        </header>

        <main>
            <section>
                <div class="centered">
                    <div class="small">
                        <figure>
                            <img src="static/figures/method-summary.svg" />
                        </figure>
                    </div>
                </div>
            </section>

            <section>
                <div class="abstract">
                    <h3>Abstract</h3>
                    <p>
                        Learned successor features provide a powerful framework for learning goal-reaching policies.
                        These representations are constructed such that similarity in the representation space predicts
                        future outcomes, allowing goal-reaching policies to be extracted. Representations learned for
                        forward inference have some practical limitations - stitching of behaviors does not arise
                        naturally with forward objectives like contrastive classification, and additional regularization
                        is required to enable valid policy extraction. In this work, we propose a new representation
                        learning objective that enables extraction of goal-reaching policies. Our key insight is that
                        rather than learning representations of the future, we should really learn representations that
                        can associate outcomes with preceding states. We show that when combined with existing
                        quasimetric network parameterization and the right invariances, these representations let us
                        learn optimal goal-reaching policies from offline data. On existing offline GCRL benchmarks, the
                        hindsight classification objective improves performance with a simpler algorithm and fewer
                        independent networks/parameters to learn relative to past methods.
                    </p>
                </div>
            </section>

            <section>
                <h2>Temporal Metric Distillation (TMD)</h2>

                <figure>
                    <div class="margin">
                        <img src="static/figures/comparisons.svg" />
                    </div>
                    <figcaption title="Figure">
                        Comparison with prior goal-conditioned RL methods. Only TMD is able to learn use quasimetric
                        architectures to learn optimal goal-reaching policies and distances with arbitrary stochastic
                        dynamics.
                    </figcaption>
                </figure>
                <div class="margin figure">
                    <img src="static/figures/algo.svg" />
                    <div class="margin">
                        <img src="static/figures/full_loss.svg" />
                        <img src="static/figures/loss_components.svg" />
                    </div>
                </div>
            </section>

            <!-- <section> -->
            <!--     <h2>Analysis</h2> -->
            <!--     <div class="theorem"> -->
            <!--         <span>Theorem 1</span> -->
            <!--         <span>Theoretical Result</span> -->
            <!--         Vivamus mi quam, vehicula quis leo commodo, efficitur dignissim diam. Mauris sollicitudin sapien -->
            <!--         sem, non sagittis mauris tincidunt in. -->
            <!--     </div> -->
            <!--     <p> -->
            <!--         Integer viverra dapibus tempor. Morbi ac orci lorem. Aliquam nec ligula ipsum. Praesent at tempor -->
            <!--         nisl. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Sed -->
            <!--         dapibus, lectus finibus volutpat dictum, ipsum tortor rhoncus nisl, eget ullamcorper turpis erat id -->
            <!--         felis. -->
            <!--     </p> -->
            <!-- </section> -->

            <!-- <section> -->
            <!--     <h2>Videos</h2> -->
            <!--     <div class="margin"> -->
            <!--         <img src="static/figures/img1.svg" /> -->
            <!--         <p class="caption"> -->
            <!--             <b>Figure.</b> -->
            <!--             Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. -->
            <!--             Vestibulum tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante. Donec eu libero -->
            <!--             sit amet quam egestas semper. Aenean ultricies mi vitae est. Mauris placerat eleifend leo. -->
            <!--             Quisque sit amet est et sapien ullamcorper pharetra. Vestibulum erat wisi, condimentum sed, -->
            <!--             commodo vitae, ornare sit amet, wisi. Aenean fermentum, elit eget tincidunt condimentum, eros -->
            <!--             ipsum rutrum orci, sagittis tempus lacus enim ac dui. Donec non enim in turpis pulvinar -->
            <!--             facilisis. Ut felis. Praesent dapibus, neque id cursus faucibus, tortor neque egestas augue, eu -->
            <!--             vulputate magna eros eu erat. Aliquam erat volutpat. Nam dui mi, tincidunt quis, accumsan -->
            <!--             porttitor, facilisis luctus, metus -->
            <!--         </p> -->
            <!--     </div> -->
            <!-- </section> -->

            <section>
                <h2>Empirical Evaluation</h2>
                <figure>
                    <div class="columns centered">
                        <div class="wide top">
                            <img src="static/figures/eval-table.svg" />
                        </div>
                        <div class="top third">
                            <figure>
                                <div class="small margin">
                                    <video autoplay loop muted playsinline>
                                        <source src="static/videos/humanoid-traj_small.mp4" type="video/mp4" />
                                    </video>
                                </div>
                                <figcaption title="Figure">
                                    Example trajectory in the <code>humanoid</code> environment.
                                </figcaption>
                            </figure>
                        </div>
                    </div>
                </figure>
                <hr />
                <figure>
                    <div class="columns centered">
                        <div class="half">
                            <img src="static/figures/ablations.svg" />
                            <div class="bottom">
                                <figcaption title="Figure">
                                    We ablate the loss components of TMD in the
                                    <code>pointmaze_teleport_stitch</code> environment.
                                </figcaption>
                            </div>
                        </div>
                        <div class="half">
                            <img src="static/figures/t_ablation.svg" />
                            <div class="bottom">
                                <figcaption title="Figure">
                                    Ablating the objectives in the $\mathcal{T}$-backup experiment.
                                </figcaption>
                            </div>
                        </div>
                    </div>
                </figure>
            </section>

            <!-- <section> -->
            <!--     <h2> -->
            <!--         ${\bf B\kern-.05em{\small I\kern-.025em B}\kern-.08em -->
            <!--         T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}$ -->
            <!--     </h2> -->
            <!--     <pre class="language-bibtex bibtex" id="bibtex" onclick="selectContent('#bibtex')"><code>@misc{TODO, -->
            <!-- TODO -->
            <!-- }</code></pre> -->
            <!-- </section> -->
        </main>
    </body>
</html>
