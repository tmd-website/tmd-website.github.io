<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="description" content="" />
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
        <script defer src="./static/js/fontawesome.all.min.js"></script>
        <link
            rel="stylesheet"
            href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-solarizedlight.min.css" />
        <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.1.0/prism-bibtex.min.js"></script>
        <link rel="stylesheet" href="static/css/index.css" />

        <title>Offline Goal Conditioned Reinforcement Learning with Temporal Distance Representations</title>

        <script
            type="text/javascript"
            id="MathJax-script"
            async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-svg.js"></script>
        <script>
            function selectContent(query) {
                var range = document.createRange()
                var selection = window.getSelection()
                var elem = document.querySelector(query)
                range.selectNodeContents(elem)
                selection.removeAllRanges()
                selection.addRange(range)
            }
        </script>
    </head>

    <body>
        <script type="text/javascript">
            window.MathJax = {
                tex: {
                    inlineMath: [
                        ["$", "$"],
                        ["\\(", "\\)"],
                    ],
                    displayMath: [
                        ["$$", "$$"],
                        ["\\[", "\\]"],
                    ],
                    processEscapes: true,
                    macros: {
                        S: "\\mathcal{S}",
                        A: "\\mathcal{A}",
                        D: "\\mathcal{D}",
                        R: "\\mathbb{R}",
                        E: "\\mathbb{E}",
                        P: "\\mathrm{P}",
                        var: "\\mathrm{Var}",
                        cov: "\\mathrm{Cov}",
                        argmin: "\\mathop{\\arg\\min}",
                        argmax: "\\mathop{\\arg\\max}",
                    },
                },
                svg: {
                    fontCache: "global",
                },
            }
        </script>
        <header>
            <h1>Offline Goal Conditioned Reinforcement Learning with Temporal Distance Representations</h1>
            <div class="authors">
                <span class="author">Vivek Myers</span>
                <span class="affil">1</span>
                <span class="author">Bill Chunyuan Zheng</span>
                <span class="affil">1</span>
                <span class="author">Benjamin Eysenbach</span>
                <span class="affil">2</span>
                <span class="author">Sergey Levine</span>
                <span class="affil">1</span>
            </div>
            <div class="notes">
                <span class="affil">1</span>
                <span class="institution">UC Berkeley</span>
                <span class="affil">2</span>
                <span class="institution">Princeton University</span>
            </div>
            <div class="links">
                <span class="link">
                    <a href="./static/pdf/tmd.pdf" target="_blank" class="button">
                        <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
                    </a>
                </span>
                <!-- <span class="link"> -->
                <!--   <a href="TODO" class="button"> -->
                <!--     <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span> -->
                <!--   </a> -->
                <!-- </span> -->
                <span class="link">
                    <a href="https://anonymous.4open.science/r/ogcrl-FCDC/" class="button">
                        <span class="icon"><i class="fas fa-code"></i></span><span>Code</span>
                    </a>
                </span>
            </div>
        </header>

        <main>
            <section>
                <div class="columns centered xspace">
                    <div class="abstract half axpad">
                        <h3>Abstract</h3>
                        <p>
                            Learned successor features provide a powerful framework for learning goal-reaching policies.
                            These representations are constructed such that similarity in the representation space
                            predicts future outcomes, allowing goal-reaching policies to be extracted. Representations
                            learned for forward inference have some practical limitations - stitching of behaviors does
                            not arise naturally with forward objectives like contrastive classification, and additional
                            regularization is required to enable valid policy extraction. In this work, we propose a new
                            representation learning objective that enables extraction of goal-reaching policies. We show
                            that when combined with existing quasimetric network parameterization and the right
                            invariances, these representations let us learn optimal goal-reaching policies from offline
                            data. On existing offline GCRL benchmarks, our representation learning objective improves
                            performance with a simpler algorithm and fewer independent networks/parameters to learn
                            relative to past methods.
                        </p>
                    </div>
                    <div class="vertical medium">
                        <figure>
                            <video autoplay loop muted playsinline>
                                <source
                                    src="https://res.cloudinary.com/dp7qzzmt2/video/upload/v1752709113/tmd-anim_bn0p88.mp4"
                                    type="video/mp4" />
                            </video>
                            <figcaption title="Figure">
                                Example trajectory in the <code>antmaze</code> environment.
                            </figcaption>
                        </figure>
                    </div>
                </div>
            </section>

            <section>
                <h2>Temporal Metric Distillation (TMD)</h2>

                <figure>
                    <div class="margin">
                        <img src="static/figures/comparisons.svg" />
                    </div>
                    <figcaption title="Figure">
                        Comparison with prior goal-conditioned RL methods. Only TMD is able to learn use quasimetric
                        architectures to learn optimal goal-reaching policies and distances with arbitrary stochastic
                        dynamics.
                    </figcaption>
                </figure>

                <div class="margin figure">
                    <img src="static/figures/algo.svg" />
                </div>
                <details>
                    <summary>Full Implementation</summary>

                    <div class="margin">
                        <img src="static/figures/full_loss.svg" />
                        <img src="static/figures/loss_components.svg" />
                    </div>
                </details>
                <p>
                    To learn a policy from this distance parameterization, we can simply use off-the-shelf policy
                    extraction method and minimize actor loss: $\pi = \min_{\pi}
                    \mathbb{E}_{\{s_{i},a_{i},s_{i}',g_{i}\}_{i=1}^{N} \sim \pi_{\beta}} \Bigl[ \sum_{i,j=1}^{N}
                    d_{\theta}\bigl((s_{i},\pi(s_{i},g_{j})),g_{j}\bigr) \Bigr]$.
                </p>
            </section>

            <!-- <section> -->
            <!--     <h2>Analysis</h2> -->
            <!--     <div class="theorem"> -->
            <!--         <span>Theorem 1</span> -->
            <!--         <span>Theoretical Result</span> -->
            <!--         Vivamus mi quam, vehicula quis leo commodo, efficitur dignissim diam. Mauris sollicitudin sapien -->
            <!--         sem, non sagittis mauris tincidunt in. -->
            <!--     </div> -->
            <!--     <p> -->
            <!--         Integer viverra dapibus tempor. Morbi ac orci lorem. Aliquam nec ligula ipsum. Praesent at tempor -->
            <!--         nisl. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Sed -->
            <!--         dapibus, lectus finibus volutpat dictum, ipsum tortor rhoncus nisl, eget ullamcorper turpis erat id -->
            <!--         felis. -->
            <!--     </p> -->
            <!-- </section> -->

            <!-- <section> -->
            <!--     <h2>Videos</h2> -->
            <!--     <div class="margin"> -->
            <!--         <img src="static/figures/img1.svg" /> -->
            <!--         <p class="caption"> -->
            <!--             <b>Figure.</b> -->
            <!--             Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. -->
            <!--             Vestibulum tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante. Donec eu libero -->
            <!--             sit amet quam egestas semper. Aenean ultricies mi vitae est. Mauris placerat eleifend leo. -->
            <!--             Quisque sit amet est et sapien ullamcorper pharetra. Vestibulum erat wisi, condimentum sed, -->
            <!--             commodo vitae, ornare sit amet, wisi. Aenean fermentum, elit eget tincidunt condimentum, eros -->
            <!--             ipsum rutrum orci, sagittis tempus lacus enim ac dui. Donec non enim in turpis pulvinar -->
            <!--             facilisis. Ut felis. Praesent dapibus, neque id cursus faucibus, tortor neque egestas augue, eu -->
            <!--             vulputate magna eros eu erat. Aliquam erat volutpat. Nam dui mi, tincidunt quis, accumsan -->
            <!--             porttitor, facilisis luctus, metus -->
            <!--         </p> -->
            <!--     </div> -->
            <!-- </section> -->

            <section>
                <h2>What Do These Invariances Mean?</h2>
                <div class="centered">
                    <div class="small">
                        <figure>
                            <img src="static/figures/method-summary.svg" />
                        </figure>
                    </div>
                </div>
                <p>
                    An ideal distance metric should obey two properties: the <i>triangle inequality</i> and
                    <i>action invariance</i>, which correspond to Bellman optimality.
                </p>
                <p>
                    The $\mathcal{T}$-invariance is represents Bellman consistency in temporal distance learning.
                    Instead of enforcing Bellman consistency straight up, we use the following objective to enforce the
                    same constraint on quasimetric architecture:
                </p>
                <p style="text-align: center">
                    $ e^{-d_{\mathrm{MRN}}\bigl(\phi(s,a),\,\psi(g)\bigr)} \;\leftarrow\; \mathbb{E}_{s'\sim
                    P(\,\cdot\mid s,a)}\Bigl[ e^{\log\gamma \;-\; d_{\mathrm{MRN}}\bigl(\psi(s'),\,\psi(g)\bigr)}
                    \Bigr].$
                </p>
                <p>
                    The $\mathcal{I}$-invariance represents how optimal value and Q functions behave: $V^{*}(s) =
                    \max_{a \in A} Q^{*}(s,a)$. To enforce this under a quasimetric architecture, we enforce the same
                    constraint by minimizing the MRN distance between the state encoder and state-action encoder:
                </p>

                <p style="text-align: center">
                    $ \mathcal{L}_{\mathcal{I}}\bigl(\phi, \psi;\{s_i,a_i,s'_i,g_i\}_{i=1}^N\bigr) = \sum_{i=1}^N
                    \sum_{j=1}^N d_{\mathrm{MRN}}\bigl(\psi(s_i),\,\phi(s_i,a_j)\bigr) $.
                </p>
            </section>

            <section>
                <h2>Empirical Evaluation</h2>
                <p>
                    We evaluate TMD on OGBench, and we observe that TMD outperforms other methods in a variety of
                    locomotion and manipulation tasks. Across a collection of <b>80 tasks</b>, we observe that TMD
                    consistently outperforms TD-based methods such as GCIQL as well as distance learning methods such as
                    QRL and CMD.
                </p>
                <figure>
                    <div class="columns centered">
                        <div class="wide top">
                            <img src="static/figures/eval-table.svg" />
                        </div>
                    </div>
                </figure>
                <hr />
                <figure>
                    <div class="columns centered">
                        <div class="half">
                            <img src="static/figures/ablations.svg" />
                            <div class="bottom">
                                <figcaption title="Figure">
                                    We ablate the loss components of TMD in the
                                    <code>pointmaze_teleport_stitch</code> environment.
                                </figcaption>
                            </div>
                        </div>
                        <div class="half">
                            <p>
                                We additionally ablate our method, demonstrating the need of all three components that
                                require a strong distance metric for policy learning.
                            </p>
                            <img src="static/figures/t_ablation.svg" />
                            <div class="bottom">
                                <figcaption title="Figure">
                                    Ablating the objectives in the $\mathcal{T}$-backup experiment.
                                </figcaption>
                            </div>
                        </div>
                    </div>
                </figure>
            </section>

            <!-- <section> -->
            <!--     <h2> -->
            <!--         ${\bf B\kern-.05em{\small I\kern-.025em B}\kern-.08em -->
            <!--         T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}$ -->
            <!--     </h2> -->
            <!--     <pre class="language-bibtex bibtex" id="bibtex" onclick="selectContent('#bibtex')"><code>@misc{TODO, -->
            <!-- TODO -->
            <!-- }</code></pre> -->
            <!-- </section> -->
        </main>
    </body>
</html>
